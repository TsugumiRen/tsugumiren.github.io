---
title: 基于熵的压缩算法：ANS
date: 2025-10-04 17:23:00 +0800
categories:
  - Algorithm
  - Compress
tags:
  - algorithm
  - compress
description: 向使用非整数比特位描述符号迈进
math: true
---
## 香农、信息论和熵

香农于 1948 年的 A mathematical theory of communication 一文中将热力学的熵引入到信息论中，以此衡量一条消息中所含的信息的“量”。具体而言，若符号 $s$ 的出现频率为 $p_s$  ，那么单个符号 $s$  的平均信息熵为：


$$
H(s) = \lg(\frac{1}{p_s})
$$

其出现频率越大，所蕴含的信息熵则越小（所以说复读机带来的信息量极低 XD）。

假设 $S$ 是消息 $m$ 中全体符号 $s$ 的集合，那么该条消息的平均信息熵为：

$$
H(m) = \sum_{s\in S}p_s\lg(\frac{1}{p_s})
$$

不难看出，当所有符号 $s$ 的出现频率均等时，$H(m)$ 取得最大值。也就是说，每个符号出现的概率约平均，对应的消息就越难以被压缩。

香农信息熵实际指出的是基于熵编码的压缩算法所能做到的压缩比的上界。熵压缩算法通常利用每个符号的出现的概率来进行压缩（譬如说出现概率大的符号则分配位数较少的符号串来编码，我们后文会介绍主流的几种），而香农的信息熵公式则指出了每个符号的最短表示位数 $H(s)$ ，对于符号 $s$ 压缩后的实际表示位数 $L_C(s)$ ，我们有 $L_C(s)\geq H(s)$ 。

## Huffman 算法

几乎每一本算法教材都会把 **Huffman** 算法当作是贪心算法应用的例子来讲述，其本身思想也很简单。粗略地讲，给定一组符号和符号的频度，通过 Huffman 算法，我们能按照频度的大小由高到低的给每个符号分配长度由短到长的前缀编码（注意不一定是严格递增的，有可能有相等的），从而使得符号频度 $f_i$  与对应前缀编码长度 $c_i$ 乘积的总和 $\sum f_i \cdot c_i$  最小。

> **前缀编码**：形如 0, 10, 110, 1110 ... 的编码，其中任一码字都不是剩余码字的前缀（比如说 0 不是 10, 110, 1110 的前缀）。注意到，在这样的编码中，我们不需要知道每个码字的长度就可以无歧义地识别出分割出每个码字。

其过程简单来讲是：

1. 将所有符号都视为带频度的（叶子）结点
2. 选取所有结点中两个频度最小的结点，使其成为一个新结点的子结点；其中新结点的频度是两子结点的频度之和；
3. 重复步骤 2. 直到只剩下最后一个子结点

获取编码则通过遍历构造出来的 Huffman 树进行，通过编码遍历路径来获取符号的编码。例如说，到达某一叶子结点的路径为（左-右-左），则可以把它编码为0-1-0。注意到，通过上述方法构造出来的 Huffman 树，频度越小的叶子结点始终拥有更长的路径长度，也即编码长度，也就满足了我们的要求。对于详细证明感兴趣的同学可以参看 CLRS 之类的书籍。

用 C++ 代码来描述就是（做了简单验证，但不保证是对的）：

```cpp
struct Node {
    shared_ptr<Node> left, right;
    int freq;
    char symbol;
    bool is_leaf;
    Node(char _s, int _f) : symbol(_s), freq(_f), is_leaf(true) {}
    Node(int _f, shared_ptr<Node> _l, shared_ptr<Node> _r) 
        : freq(_f), left(_l), right(_r), is_leaf(false) {}
};

shared_ptr<Node> construct_huffman_tree(const unordered_map<char,int>& symbol_freq) {
    auto comp = [](shared_ptr<Node> lhs, shared_ptr<Node> rhs){return lhs->freq < rhs->freq;};
    priority_queue<shared_ptr<Node>, vector<shared_ptr<Node>>, decltype(comp)> node_heap(comp);
    // init minimum heap
    for (auto [sym, freq]:symbol_freq) {
        node_heap.push(make_shared<Node>(sym, freq));
    }

    // construct tree until only one node greedily
    while (node_heap.size() >= 2) {
        auto node1 = node_heap.top();
        node_heap.pop();
        auto node2 = node_heap.top();
        node_heap.pop();

        auto new_node = make_shared<Node>(node1->freq + node2->freq, node1, node2);
        node_heap.push(new_node);
    }

    return node_heap.top();
}

unordered_map<char, string> get_encoding(shared_ptr<Node> head) {
    unordered_map<char,string> encode_tbl;
    function<void(shared_ptr<Node>, string&)> dfs = [&](shared_ptr<Node> cur, string& code) {
      if (cur->is_leaf) {
        encode_tbl[cur->symbol] = code;
        return;
      }

      if (cur->left) {
          code.append("0");
          dfs(cur->left, code);
          code.pop_back();
      }

      if (cur->right) {
          code.append("1");
          dfs(cur->right, code);
          code.pop_back();
      }
    };

    string s;
    dfs(head, s);
    return encode_tbl;
}
```

## 算术编码

Huffman 编码提供了一种能按照依照熵分配符号编码的方式，在符号出现频率差别巨大的情况下，其能够获得较大的压缩比例。但正如我们在文章一开始所提到的一样，由于 Huffman 只能将符号固定地表示为整数位数二进制串，因此对于一个概率出现概率为 $p$ 的符号 $s$，编码后其实际的位数为 $\lceil-p\lg p\rceil$ 而不是 $-p\lg p$。

**算术编码**则采用新的思路进行编码，其并不是对单个符号按频次进行编码后应用到整个符号串；而是将一整个符号串不唯一地对应到实数空间内，并选取实数空间内二进制串最少的实数作为符号串的编码，从而尽可能地使编码后的位数接近 $-p\lg p$。

具体地说，我们假定每个符号出现的概率为 $p_i$ ，那么我们有如下算法：

1. 初始化 $low = 0, high = 1$
2. 对于每一个符号 $s_i$，更新 $[low, high]$ 为 $[low + (high-low) \times \sum{p_{i-1}},\ low + (high-low) \times \sum{p_i}]$ 
3. 从最终的 $[low, high]$ 中选取一个表示位数最小的实数作为该符号串的表示

如果我们想要从最终的表示中还原出原符号串的话，只需要遵循相反的步骤即可。

我们不妨用一个例子来看一下其运作的步骤，假设我们有一符号系统，其只含有 A、B、C 三个符号，且每个符号的出现概率为 0.6，0.3，0.1。给定一个符号串 ABA，其将按照以下的方式进行编码：

1. 初始化区间为\[0,1)，并按照符号出现概率分配区间给它们。

| A   | \[0, 0.6)   |
| --- | ----------- |
| B   | \[0.6, 0.9) |
| C   | \[0.9, 1)   |

2. 因为第一个符号是 A，因此我们最终的结果会落在 \[0, 0.6) 区间内。在 \[0, 0.6) 中，我们同样按照符号各自出现的概率分配它们各自占有的区间。

| AA  | \[0, 0.36)    |
| --- | ------------- |
| AB  | \[0.36, 0.54) |
| AC  | \[0.54, 0.6)  |

3. 因为第二个符号是 B，因此我们最终的结果会落在 \[0.36, 0.54) 中。同样地，我们分配第三个符号对应地区间。

| ABA | \[0.36, 0.468)  |
| --- | --------------- |
| ABB | \[0.468, 0.522) |
| ABC | \[0.522, 0.6)   |

4. 最后的符号是 A，因此我们最终地结果落在了 \[0.36, 0.468] 中，这个范围中的任何一个实数都可以成为 ABA 的表示方式，我们在其中寻找一个占用二进制位数最少的即可。在这个例子中，占用二进制位数最少的是 0.375，其对应二进制表示是 0.011，只需要三位即可。

## ANS

主播主播，虽然算术编码压缩率比 Huffman 更高，但是大量的浮点乘法运算也带来了额外的性能开销。有没有一种编码方法，能够获得近似算术编码压缩率的同时，还能拥有近似 Huffman 的运算效率呢？

有的有的！**ANS**（Asymmetric Numeral System，非对称数字系统）就是这样一种技术。ANS 这个名字听起来有一点抽象，现在我们来解释一下。数字系统（Numeral System）指的是用某种方式来表示数字的系统，例如在二进制的数字系统中，我们用 $0$ 和 $1$ 这两个符号组成符号串来表征任意一个自然数。如果定义自然数 $x$ 携带的信息量为 $\text{lg}(x)$，那么在对称（Symmetric）的二进制数字系统中，每在已有符号串中增加一个 $0$ 或 $1$，该符号串的信息量就会增加 1，这要求符号的出现概率 $\text{Pr}(0)=\text{Pr}(1)$。反过来，如果在已有符号串中增加一个符号 $s$，但增加的信息量依照符号 $s$ 的不同而不同，那么这个数字系统是非对称（Asymmetric）的。

ANS 实现压缩的方法便在于其非对称性，在 ANS 中，每增加一个符号 $s$，编码串增加的信息量近似为 $\lg(1/p_s)$，也就是说，近似于该符号的信息熵。下面我们来看看如何实现。

### 形式化描述

对于向已有编码串 $x$ （一般而言这是一个自然数）中新增加一个符号 $s$ 的函数（编码函数），我们定义为 $C(s,x)$：

$$ x^\prime = C(s,x) $$
而对于解码函数 $D(x^\prime)$，其是 $C(s,x)$ 的逆函数，定义为：

$$D(x^\prime) = D(C(s,x)) = (s,x)$$

其中的 $s$ 由另外一个函数还原出来，我们把这个符号扩展函数（Symbol Spread Function）定义为 $\bar{s}(x^\prime)$，也即：

$$D(x^\prime) = D(C(s,x)) = (\bar{s}(x^\prime), x)=(s,x)$$

如果要实现增加符号 $s$，编码串信息量增加 $\lg(1/p_s)$，我们需要使得 $x^\prime=x/p_s$。为了实现这一点，我们可以限定 $\bar{s}(x^\prime)$，使得对于任意 $s$ 和 $C(s,x)=x^\prime$  有：

$$\bar{s}(x^\prime) = s\ \text{and}\ |{0 ≤ y < x^\prime : \bar{s}(y) = s}| = x$$

总结下来，只要编码函数 $C(s,x)$、解码函数 $D(x^\prime)$ 和符号扩展函数 $\bar{s}(x^\prime)$ 满足下式，我们就能形成一个能够逼近基于熵压缩算法上界的 ANS：

$$
\begin{align}
x^\prime = C(s,x)\ &:\ \bar{s}(x^\prime) = s\ \text{and}\ |{0 ≤ y < x^\prime : \bar{s}(y) = s}| = x \\ 
D(x^\prime) &= (\bar{s}(x^\prime),\ |{0 ≤ y < x^\prime : \bar{s}(y) = s}|)
\end{align}
$$

### rANS

ANS 有多种实现，在这里我们介绍一种被称为 rANS（range-ANS）的实现。rANS 将连续的 $n$ 个自然数按照符号的概率分配给各个符号，因此被称为 range-ANS。例如，有 $A:6,\ B:3,\ C:1$ ，那么其就可能是在 $[x, x+9]$ 中，$[x, x+5]$ 分配给 A，$[x+6, x+8]$ 分配给 B，$x+9$ 分配给 C。

一般而言，为了运算的效率（利用移位代替乘法）起见，我们会选用大小为 $2^n$ 的范围。对应的编码函数 $C(s,x)$、解码函数 $D(x^\prime)$ 和符号扩展函数 $\bar{s}(x^\prime)$ 为：

$$
\begin{align}
C(s,x) &= \lfloor \frac{x}{f_s} \rfloor \cdot 2^n + c_s+x\ \text{mod}\ f_s\\
\bar{s}(x) &= s_i\quad s.t.\ c_s \leq x\ \text{mod}\ 2^n\lt c_{s+1}\\
D(x^\prime) &= (s,\ f_s\cdot\lfloor\frac{x^\prime}{2^n}\rfloor+x^\prime\ \text{mod}\ 2^n - c_s)
\end{align}
$$

其中 $c_s=f_0+f_1+\dots+f_{s-1}$。

感兴趣的读者可以自己动手试试，在这里主播懒癌发作就不花篇幅展开了。

在实践过程中，我们很容易就会得到一个非常巨大的 $x$，最简单的方法是在所有压缩完成后再将这个 $x$ 转化为二进制存储，但这会带来中间存储 $x$ 的开销；此外，这种方式也没有办法支持流式的压缩与解压缩。因此，我们有另外一种边读取原符号串边进行压缩的方法，反之亦可。其过程如下所示（引自[这篇文章](https://bjlkeng.io/posts/lossless-compression-with-asymmetric-numeral-systems/)）。

```Python
MASK = 2**M - 1
BOUND = 2**(2*M) - 1

def encoding_step():
    x_test = (x / f[s]) << n + (x % f[s]) + c[s]
    if (x_test > BOUND):
        writeBits(x & MASK)
        x = x >> M
    x = (x / f[s]) << n + (x % f[s]) + c[s]

def decode_step():
    s = symbol[x & MASK]
    writeSymbol(s)
    x = f[s]*(x >> n) + (x & MASK) - c[s]
    if (x < 2**M):
        x = x << M + readBits()
```

其中的 M 决定了每次 `read/writeBits` 时操作的位数，在原论文中，作者取了16即16 bit。与 Mask 作和运算代替了取余操作，移位操作则代替了原式中的乘法操作，这也是 rANS 运行效率较高的原因。

在算法的编码过程中，我们始终将 $x$ 维持在 $[2^M, 2^{2M} -1]$ 之间，如果超出 `BOUND` 则将低 $M$ 位写入；解码过程则与之相反。

### 其他变种

实际上 tANS（table-ANS）比 rANS 更为常用，其预先计算好一张表格，之后只需要查表就可进行编解码操作，用预计算的开销为代价省去了加解密过程中的计算开销。另外，也有基于状态机的实现，其效率更高。

对 tANS 感兴趣的朋友可以参考《数据压缩入门》一书或者是 [The use of asymmetric numeral systems as an accurate replacement for Huffman coding](http://ieeexplore.ieee.org/document/7170048/) 一论文

